import { ipcMain, BrowserWindow } from 'electron';
import { getDB, getChatSessions, upsertChatSummary, getMemoriesForSession, getMemoryRecordsForSession, getMasterMemory, updateMasterMemory, getAllChatSummaries, upsertEntity, addEntityFact, updateEntitySummary, getEntities, getEntityDetails, upsertEntitySession, rebuildEntityEdgesForSession, getEntityGraph, rebuildEntityEdgesForAllSessions, deleteEntity, deleteMemory, getEntitiesForSession, getDashboardStats, getUserProfile, saveUserProfile, UserProfile, createRagConversation, getRagConversations, getRagConversation, deleteRagConversation, addRagMessage, getRagMessages, updateRagConversationTitle, getSettings, saveSetting, getSetting } from './database';
import { embeddings } from './embeddings';
import { getPermissionStatus, requestAccessibilityPermission, requestScreenRecordingPermission, openAccessibilitySettings, openScreenRecordingSettings } from './permissions';
import { getPrompt, getAllPromptDefs, resetPrompt, getPromptTemplate } from './prompts';
// import { llm } from './llm'; // Moved to dynamic import to support ESM

// Incrementally update master memory with a new conversation summary
// This approach keeps context bounded by only processing current master + new summary
async function updateMasterMemoryIncremental(newSummary: string): Promise<string | null> {
    console.log('[IPC] Starting incremental master memory update...');
    const currentMasterData = getMasterMemory();
    const currentMaster = currentMasterData?.content;

    // If no existing master memory, create initial one from just this summary
    if (!currentMaster || currentMaster.trim().length === 0) {
        console.log('[IPC] No existing master memory, creating from new summary only');
        const prompt = getPrompt('masterMemory.initial', { SUMMARY: newSummary });

        try {
            const { llm } = await import('./llm');
            const initialMaster = await llm.chat(prompt, [], 600000, 4096, 'Master Memory');
            updateMasterMemory(initialMaster);
            console.log('[IPC] Initial master memory created');
            return initialMaster;
        } catch (e) {
            console.error('[IPC] Failed to create initial master memory:', e);
            return null;
        }
    }

    // Incremental update: merge new summary with existing master (no truncation - allow growth)
    const prompt = getPrompt('masterMemory.incremental', { CURRENT_MASTER: currentMaster, NEW_SUMMARY: newSummary });

    try {
        const { llm } = await import('./llm');
        const updatedMaster = await llm.chat(prompt, [], 600000, 4096, 'Master Memory');
        updateMasterMemory(updatedMaster);
        console.log('[IPC] Master memory updated incrementally');
        return updatedMaster;
    } catch (e) {
        console.error('[IPC] Failed to update master memory incrementally:', e);
        return null;
    }
}

// Full regeneration using map-reduce:
// Phase 1 (map): Split summaries into chunks, generate a partial summary for each
// Phase 2 (reduce): Merge all partial summaries into the final master memory
// This avoids the growing-prompt problem and minimizes LLM calls
async function regenerateMasterMemoryFull(): Promise<string | null> {
    const summaries = getAllChatSummaries();
    console.log(`[IPC] regenerateMasterMemoryFull called, found ${summaries.length} summaries`);

    if (summaries.length === 0) {
        console.log('[IPC] No summaries found — clearing master memory');
        updateMasterMemory('');
        return null;
    }

    const { llm } = await import('./llm');

    const sendProgress = (current: number, total: number) => {
        BrowserWindow.getAllWindows().forEach(win => {
            win.webContents.send('master-memory:progress', { current, total });
        });
    };

    // If few enough summaries, do it in one shot
    const allText = summaries.map((s, i) => `[Session ${i + 1}]\n${s.summary}`).join('\n\n---\n\n');
    if (allText.length < 60000) {
        console.log(`[IPC] All summaries fit in one prompt (${allText.length} chars), single-shot`);
        sendProgress(0, 1);
        const prompt = getPrompt('masterMemory.batchFirst', { BATCH_TEXT: allText });
        const master = await llm.chat(prompt, [], 600000, 4096, 'Master Memory (batch)');
        updateMasterMemory(master);
        sendProgress(1, 1);
        console.log(`[IPC] Master memory regenerated single-shot (${master.length} chars)`);
        return master;
    }

    // Map-reduce for large sets
    // Phase 1: split into chunks of ~50K chars each, generate partial summaries
    const CHUNK_MAX_CHARS = 50000;
    const chunks: string[][] = [[]];
    let currentChunkSize = 0;

    for (const s of summaries) {
        if (currentChunkSize + s.summary.length > CHUNK_MAX_CHARS && chunks[chunks.length - 1].length > 0) {
            chunks.push([]);
            currentChunkSize = 0;
        }
        chunks[chunks.length - 1].push(s.summary);
        currentChunkSize += s.summary.length;
    }

    const totalSteps = chunks.length + 1; // chunks + 1 merge step
    console.log(`[IPC] Map-reduce: ${chunks.length} chunks + 1 merge = ${totalSteps} steps`);
    sendProgress(0, totalSteps);

    const partials: string[] = [];

    for (let i = 0; i < chunks.length; i++) {
        const batchText = chunks[i].map((s, j) => `[Session ${j + 1}]\n${s}`).join('\n\n---\n\n');
        const prompt = getPrompt('masterMemory.batchFirst', { BATCH_TEXT: batchText });
        console.log(`[IPC] Phase 1 chunk ${i + 1}/${chunks.length}: ${batchText.length} chars input, prompt ${prompt.length} chars`);

        try {
            const partial = await llm.chat(prompt, [], 600000, 2048, 'Master Memory (batch)');
            partials.push(partial);
            console.log(`[IPC] Chunk ${i + 1} done: ${partial.length} chars output`);
            sendProgress(i + 1, totalSteps);
        } catch (e) {
            console.error(`[IPC] Chunk ${i + 1} FAILED:`, e);
            throw e;
        }
    }

    // Phase 2: merge all partials into final master memory
    console.log(`[IPC] Phase 2: merging ${partials.length} partial summaries...`);
    const mergeInput = partials.map((p, i) => `[Part ${i + 1}]\n${p}`).join('\n\n---\n\n');
    const mergePrompt = getPrompt('masterMemory.merge', { PARTIAL_SUMMARIES: mergeInput });
    console.log(`[IPC] Merge prompt: ${mergePrompt.length} chars`);

    const finalMaster = await llm.chat(mergePrompt, [], 600000, 4096, 'Master Memory (batch)');
    updateMasterMemory(finalMaster);
    sendProgress(totalSteps, totalSteps);
    console.log(`[IPC] Master memory regenerated via map-reduce (${finalMaster.length} chars)`);
    return finalMaster;
}

// Main entry point - full regeneration
async function regenerateMasterMemory(): Promise<string | null> {
    return regenerateMasterMemoryFull();
}

function safeParseJson<T>(input: string, fallback: T): T {
    try {
        const clean = input.replace(/```json\n?|\n?```/g, '').trim();
        return JSON.parse(clean) as T;
    } catch {
        return fallback;
    }
}

const STOPWORDS = new Set([
    'the', 'and', 'for', 'with', 'from', 'that', 'this', 'what', 'know', 'about', 'your', 'you', 'me', 'my', 'all', 'do',
    'are', 'was', 'were', 'been', 'being', 'have', 'has', 'had', 'will', 'would', 'should', 'could', 'can', 'may', 'might'
]);

function tokenizeQuery(query: string, maxTokens: number = 6): string[] {
    const tokens = query
        .toLowerCase()
        .split(/\s+/)
        .map(t => t.replace(/[^a-z0-9_-]/g, ''))
        .filter(t => t.length >= 3)
        .filter(t => !STOPWORDS.has(t));
    return Array.from(new Set(tokens)).slice(0, maxTokens);
}

function clipText(text: string, maxLength: number): string {
    if (!text) return '';
    if (text.length <= maxLength) return text;
    return text.slice(0, Math.max(0, maxLength - 1)) + '…';
}

function isTrivialMessage(text: string): boolean {
    const normalized = (text || '').trim();
    if (normalized.length === 0) return true;
    if (normalized.length < 20) {
        if (/^(hi|hello|hey|thanks|thank you|ok|okay|sure|yes|no|cool|great|nice|good|fine|bye|see ya|yep|nope)[!.]?$/i.test(normalized)) {
            return true;
        }
    }
    return false;
}

async function insertMemoryRecord(params: {
    content: string;
    name?: string | null;
    rawText?: string | null;
    sourceApp?: string | null;
    sessionId?: string | null;
    messageId?: number | null;
}): Promise<number | null> {
    const db = getDB();
    const content = (params.content || '').trim();
    if (!content) return null;

    if (params.messageId) {
        const existing = db.prepare('SELECT id FROM memories WHERE message_id = ? LIMIT 1').get(params.messageId) as { id: number } | undefined;
        if (existing?.id) return existing.id;
    }

    if (params.sessionId) {
        const existing = db.prepare('SELECT id FROM memories WHERE session_id = ? AND content = ? LIMIT 1').get(params.sessionId, content) as { id: number } | undefined;
        if (existing?.id) return existing.id;
    }

    let vectorJson = '[]';
    try {
        const vector = await embeddings.generateEmbedding(content);
        vectorJson = JSON.stringify(vector);
    } catch (e) {
        console.error('Failed to generate embedding for memory record:', e);
    }

    const stmt = db.prepare('INSERT INTO memories (content, name, raw_text, source_app, session_id, embedding, message_id) VALUES (?, ?, ?, ?, ?, ?, ?)');
    const info = stmt.run(
        content,
        params.name || null,
        params.rawText || null,
        params.sourceApp || null,
        params.sessionId || null,
        vectorJson,
        params.messageId || null
    );
    const memoryId = Number(info.lastInsertRowid || 0) || null;
    
    // Send notification about new memory
    if (memoryId) {
        BrowserWindow.getAllWindows().forEach(win => {
            win.webContents.send('notification:new-memory', {
                sessionId: params.sessionId || null,
                memoryContent: content.slice(0, 100) + (content.length > 100 ? '...' : '')
            });
        });
    }
    
    return memoryId;
}

export async function evaluateAndStoreMemoryForMessage(params: {
    sessionId: string;
    appName: string;
    role: string;
    content: string;
    messageId?: number | null;
}): Promise<void> {
    const role = (params.role || 'unknown').toLowerCase();
    const text = (params.content || '').trim();
    if (!text || isTrivialMessage(text)) return;

    // Get strictness setting
    const strictness = getSetting<'lenient' | 'balanced' | 'strict'>('memoryStrictness', 'balanced');

    const prompt = getPrompt(`memoryFilter.${strictness}`, { ROLE: role, MESSAGE: text });

    // Minimum content length filter: skip very short messages
    if (role === 'user' && text.length < 30) return;
    if (role === 'assistant' && text.length < 50) return;

    try {
        const { llm } = await import('./llm');
        const response = await llm.chat(prompt, [], 300000, 2048, 'Memory Filter');
        const parsed = safeParseJson<{ store: boolean; name?: string; memory?: string }>(response, { store: false });
        if (!parsed.store) return;
        const memoryText = (parsed.memory || '').trim();
        const memoryName = (parsed.name || '').trim() || null;
        if (!memoryText) return;
        if (memoryText.split(/\s+/).length < 4) return;
        if (memoryText.length > 280) return;

        // Post-LLM filter: skip memories matching generic / low-value patterns
        const genericPatterns = [
            /^the user (asked|said|mentioned|wanted|is|was|has|had)\b/i,
            /^(this|that|it) (is|was|seems|appears|looks)\b/i,
            /^(a|an|the) (good|great|nice|common|typical|standard|normal)\b/i,
            /\b(in general|generally speaking|as usual|as always)\b/i,
        ];
        if (genericPatterns.some(p => p.test(memoryText))) return;

        // Post-LLM filter: skip near-duplicates via substring check against existing session memories
        if (params.sessionId) {
            const existingMemories = getMemoryRecordsForSession(params.sessionId);
            const memLower = memoryText.toLowerCase();
            const isDuplicate = existingMemories.some((m: any) => {
                const existing = (m.content || '').toLowerCase();
                return existing === memLower || existing.includes(memLower) || memLower.includes(existing);
            });
            if (isDuplicate) return;
        }

        await insertMemoryRecord({
            content: memoryText,
            name: memoryName,
            rawText: text,
            sourceApp: params.appName,
            sessionId: params.sessionId,
            messageId: params.messageId || null
        });
    } catch (e) {
        console.error('[IPC] Memory evaluation failed:', e);
    }
}

async function extractEntitiesForSession(sessionId: string): Promise<void> {
        const memories = getMemoryRecordsForSession(sessionId);
        if (!memories || memories.length === 0) return;

        // Get strictness setting
        const strictness = getSetting<'lenient' | 'balanced' | 'strict'>('entityStrictness', 'balanced');

        const memoryText = memories.map((m: any) => `- ${m.content}`).join('\n');
        
        const prompt = getPrompt(`entityExtraction.${strictness}`, { MEMORY_TEXT: memoryText });

    try {
        const { llm } = await import('./llm');
        const response = await llm.chat(prompt, [], 300000, 2048, 'Entity Extraction');
        const parsed = safeParseJson<{ entities: { name: string; type?: string; facts?: string[] }[] }>(response, { entities: [] });

        if (!parsed.entities || parsed.entities.length === 0) return;

        // Blocklist of very common short entity names that are too generic
        const ENTITY_BLOCKLIST = new Set([
            'api', 'app', 'web', 'url', 'css', 'sql', 'cli', 'ide', 'ui', 'ux',
            'html', 'http', 'json', 'xml', 'yaml', 'code', 'data', 'file', 'bug',
            'server', 'client', 'database', 'frontend', 'backend', 'website',
            'user', 'admin', 'test', 'dev', 'prod', 'staging'
        ]);

        const touchedEntityIds = new Set<number>();
        for (const entity of parsed.entities) {
            const name = (entity.name || '').trim();
            if (!name) continue;
            const type = (entity.type || 'Unknown').trim() || 'Unknown';
            const facts = Array.isArray(entity.facts) ? entity.facts.filter(Boolean).map(f => f.trim()).filter(Boolean) : [];

            // Min name length: 3 chars
            if (name.length < 3) continue;
            if (facts.length === 0) continue;
            // Skip blocklisted generic names
            if (ENTITY_BLOCKLIST.has(name.toLowerCase())) continue;

            const entityId = upsertEntity(name, type);
            if (!entityId) continue;
            touchedEntityIds.add(entityId);
            upsertEntitySession(entityId, sessionId);

            const newFacts: string[] = [];
            for (const fact of facts) {
                const inserted = addEntityFact(entityId, fact, sessionId);
                if (inserted) newFacts.push(fact);
            }

            // Send notification about new entity with facts
            if (newFacts.length > 0) {
                BrowserWindow.getAllWindows().forEach(win => {
                    win.webContents.send('notification:new-entity', {
                        entityId,
                        entityName: name,
                        entityType: type,
                        factsCount: newFacts.length
                    });
                });
            }

            if (newFacts.length === 0) continue;

            const details = getEntityDetails(entityId) as { entity?: { summary?: string } } | null;
            const existingSummary = details?.entity?.summary || '';

            const summaryPrompt = getPrompt('entitySummary', {
                NAME: name,
                TYPE: type,
                EXISTING_SUMMARY: existingSummary || '(none)',
                NEW_FACTS: '- ' + newFacts.join('\n- '),
            });

            try {
                const updatedSummary = await llm.chat(summaryPrompt, [], 300000, 2048, 'Entity Summary');
                if (updatedSummary && updatedSummary.trim()) {
                    updateEntitySummary(entityId, updatedSummary.trim());
                }
            } catch (e) {
                console.error('[IPC] Failed to update entity summary:', e);
            }
        }

        if (touchedEntityIds.size > 1) {
            rebuildEntityEdgesForSession(sessionId);
        }
    } catch (e) {
        console.error('[IPC] Entity extraction failed:', e);
    }
}


export async function summarizeSession(sessionId: string): Promise<string | null> {
    const memories = getMemoriesForSession(sessionId);
    if (!memories || memories.length === 0) return null;

    const conversationText = memories.map((m: any) => `[${m.role || 'unknown'}]: ${m.content}`).join('\n');
    const prompt = getPrompt('sessionSummary', { CONVERSATION_TEXT: conversationText });

    try {
        const { llm } = await import('./llm');
        const summary = await llm.chat(prompt, [], 120000, 2048, 'Session Summary');
        upsertChatSummary(sessionId, summary);

        // Extract and update entity memory (non-blocking — don't fail the summary if these error)
        try {
            await extractEntitiesForSession(sessionId);
        } catch (entityErr) {
            console.error('[IPC] Entity extraction failed (non-fatal):', entityErr);
        }

        // Incrementally update master memory with the new summary
        try {
            await updateMasterMemoryIncremental(summary);
        } catch (masterErr) {
            console.error('[IPC] Master memory incremental update failed (non-fatal):', masterErr);
        }

        return summary;
    } catch (e) {
        console.error("Failed to summarize session:", e);
        throw e;
    }
}

export function setupIPC() {
  const db = getDB();

  ipcMain.handle('db:get-memories', (_, limit: number = 50, appName?: string) => {
    let query = 'SELECT * FROM memories ';
    const params: any[] = [];
    
    if (appName && appName !== 'All') {
        query += 'WHERE source_app LIKE ? ';
        params.push(`%${appName}%`);
    }
    
    query += 'ORDER BY created_at DESC LIMIT ?';
    params.push(limit);
    
    const stmt = db.prepare(query);
    // SQLite stores timestamps as UTC strings "YYYY-MM-DD HH:MM:SS" by default with CURRENT_TIMESTAMP
    // To ensure JS treats them as UTC, we might need to append 'Z' or standardise.
    // However, simplest is to let frontend handle "UTC" assumption.
    return stmt.all(...params);
  });

  ipcMain.handle('db:add-memory', async (_, content: string, source: string = 'user-input', sessionId?: string) => {
    // Generate embedding
    let vectorJson = '[]';
    try {
       const vector = await embeddings.generateEmbedding(content);
       vectorJson = JSON.stringify(vector);
    } catch (e) {
       console.error("Failed to generate embedding:", e);
    }
    
    // Check if we can update an existing session
    if (sessionId) {
        // Look for a recent memory (e.g. last 12 hours) with this session_id
        const existing = db.prepare('SELECT id FROM memories WHERE session_id = ? AND created_at > datetime("now", "-12 hours") ORDER BY id DESC LIMIT 1').get(sessionId) as {id: number} | undefined;
        
        if (existing) {
             console.log(`Updating existing memory session ${sessionId} (ID: ${existing.id})`);
             const stmt = db.prepare('UPDATE memories SET content = ?, embedding = ?, created_at = CURRENT_TIMESTAMP WHERE id = ?');
             stmt.run(content, vectorJson, existing.id);
             return { id: existing.id, updated: true };
        }
    }

    const stmt = db.prepare('INSERT INTO memories (content, source_app, session_id, embedding) VALUES (?, ?, ?, ?)');
    const info = stmt.run(content, source, sessionId || null, vectorJson);
    return { id: info.lastInsertRowid };
  });

ipcMain.handle('db:search-memories', async (_, query: string) => {
    try {
        const queryVector = await embeddings.generateEmbedding(query);
        const vecStr = JSON.stringify(queryVector);
        
        const stmt = db.prepare(`
          SELECT *, cosine_similarity(embedding, ?) as score 
          FROM memories 
          WHERE embedding IS NOT NULL AND embedding != '[]'
          ORDER BY score DESC 
          LIMIT 20
        `);
        return stmt.all(vecStr);
    } catch (e) {
        console.error("Vector search failed, falling back to FTS", e);
        const stmt = db.prepare(`
          SELECT memories.* 
          FROM memories 
          JOIN memory_fts ON memories.id = memory_fts.rowid 
          WHERE memory_fts MATCH ? 
          LIMIT 20
        `);
        return stmt.all(query);
    }
  });
  
  ipcMain.handle('db:get-stats', () => {
      const count = db.prepare('SELECT COUNT(*) as count FROM memories').get();
      return count;
  });

  ipcMain.handle('db:get-dashboard-stats', () => {
      return getDashboardStats();
  });

  ipcMain.handle('llm:extract', async (_, text: string) => {
      try {
        const { llm } = await import('./llm');
        const response = await llm.chat(`Analyze the following text and extract a summary and key topics. Return JSON only with keys: summary, topic, entities. Text: "${text}"`, [], 300000, 2048, 'Extract');
        
        // Basic cleanup if the model returns markdown code blocks
        const cleanJson = response.replace(/```json\n?|\n?```/g, '').trim();
        
        return JSON.parse(cleanJson);
      } catch (e) {
         console.error("LLM Extraction failed:", e);
         // Fallback
         return {
            summary: text.slice(0, 50) + "...",
            topic: "General (Fallback)",
            entities: []
         };
      }
  });

  ipcMain.handle('llm:status', async () => {
      const { llm } = await import('./llm');
      return {
          ready: llm.isReady()
      };
  });

  ipcMain.handle('rag:chat', async (_, query: string, appName?: string, conversationHistory?: { role: string; content: string }[]) => {
      const db = getDB();
      const tokens = tokenizeQuery(query);
      const ftsQuery = tokens.length > 0 ? tokens.join(' OR ') : query;

      let memories: any[] = [];
      try {
          const queryVector = await embeddings.generateEmbedding(query);
          const vecStr = JSON.stringify(queryVector);
          const params: any[] = [vecStr];
          let memoryQuery = `
            SELECT *, cosine_similarity(embedding, ?) as score
            FROM memories
            WHERE embedding IS NOT NULL AND embedding != '[]'
          `;
          if (appName && appName !== 'All') {
              memoryQuery += ` AND source_app LIKE ? `;
              params.push(`%${appName}%`);
          }
          memoryQuery += ` ORDER BY score DESC LIMIT 12`;
          memories = db.prepare(memoryQuery).all(...params);
          memories = memories.filter((m: any) => typeof m.score !== 'number' || m.score >= 0.2);
      } catch (e) {
          console.error('[RAG] Vector search failed, falling back to FTS', e);
          const params: any[] = [];
          let fallbackQuery = `
            SELECT memories.*
            FROM memories
            JOIN memory_fts ON memories.id = memory_fts.rowid
            WHERE memory_fts MATCH ?
          `;
          params.push(query);
          if (appName && appName !== 'All') {
              fallbackQuery += ` AND memories.source_app LIKE ? `;
              params.push(`%${appName}%`);
          }
          fallbackQuery += ` LIMIT 12`;
          memories = db.prepare(fallbackQuery).all(...params);
      }

            const messageParams: any[] = [ftsQuery];
            let messageQuery = `
                SELECT m.id, m.conversation_id, m.role, m.content, m.created_at, c.title, c.app_name,
                             bm25(message_fts) as score
                FROM message_fts
                JOIN messages m ON message_fts.rowid = m.id
                JOIN conversations c ON c.id = m.conversation_id
                WHERE message_fts MATCH ?
            `;
            if (appName && appName !== 'All') {
                    messageQuery += ` AND c.app_name LIKE ? `;
                    messageParams.push(`%${appName}%`);
            }
            messageQuery += ` ORDER BY score ASC LIMIT 12`;
            const messages = db.prepare(messageQuery).all(...messageParams);

            const summaryParams: any[] = [ftsQuery];
            let summaryQuery = `
                SELECT cs.session_id, cs.summary, c.title, c.app_name, c.updated_at,
                             bm25(summary_fts) as score
                FROM summary_fts
                JOIN chat_summaries cs ON summary_fts.rowid = cs.rowid
                JOIN conversations c ON c.id = cs.session_id
                WHERE summary_fts MATCH ?
            `;
            if (appName && appName !== 'All') {
                    summaryQuery += ` AND c.app_name LIKE ? `;
                    summaryParams.push(`%${appName}%`);
            }
            summaryQuery += ` ORDER BY score ASC LIMIT 8`;
            const summaries = db.prepare(summaryQuery).all(...summaryParams);

            const entityParams: any[] = [ftsQuery];
            let entityQuery = `
                SELECT e.id, e.name, e.type, e.summary, e.updated_at,
                             bm25(entity_fts) as score
                FROM entity_fts
                JOIN entities e ON entity_fts.rowid = e.id
                WHERE entity_fts MATCH ?
            `;
            if (appName && appName !== 'All') {
                    entityQuery += `
                        AND e.id IN (
                            SELECT es.entity_id
                            FROM entity_sessions es
                            JOIN conversations c ON c.id = es.session_id
                            WHERE c.app_name LIKE ?
                        )
                    `;
                    entityParams.push(`%${appName}%`);
            }
            entityQuery += ` ORDER BY score ASC LIMIT 8`;
            const entities = db.prepare(entityQuery).all(...entityParams);

            const factParams: any[] = [ftsQuery];
            let factQuery = `
                SELECT f.fact, f.created_at, f.source_session_id, e.name, e.type,
                             bm25(entity_fact_fts) as score
                FROM entity_fact_fts
                JOIN entity_facts f ON entity_fact_fts.rowid = f.id
                JOIN entities e ON e.id = f.entity_id
                WHERE entity_fact_fts MATCH ?
            `;
            if (appName && appName !== 'All') {
                    factQuery += ` AND f.source_session_id IN (SELECT id FROM conversations WHERE app_name LIKE ?) `;
                    factParams.push(`%${appName}%`);
            }
            factQuery += ` ORDER BY score ASC LIMIT 8`;
            const entityFacts = db.prepare(factQuery).all(...factParams);

    const master = getMasterMemory();
    const masterContent = master?.content || '';
    const masterRelevant = tokens.length > 0 && tokens.some(t => masterContent.toLowerCase().includes(t));
    const masterFallback = !masterRelevant && (!memories.length && !messages.length && !summaries.length && !entities.length && !entityFacts.length);

      const memoryLines = memories.slice(0, 6).map((m: any, idx: number) =>
          `- [Memory ${idx + 1}] (${m.source_app || 'Unknown'} | ${m.created_at}): ${clipText(m.content, 500)}`
      ).join('\n');

      const messageLines = messages.slice(0, 6).map((m: any, idx: number) =>
          `- [Message ${idx + 1}] (${m.app_name || 'Unknown'} | ${m.title || 'Untitled'} | ${m.created_at}) ${m.role}: ${clipText(m.content, 400)}`
      ).join('\n');

      const summaryLines = summaries.slice(0, 6).map((s: any, idx: number) =>
          `- [Summary ${idx + 1}] (${s.app_name || 'Unknown'} | ${s.title || 'Untitled'}): ${clipText(s.summary, 600)}`
      ).join('\n');

      const entityLines = entities.slice(0, 6).map((e: any, idx: number) =>
          `- [Entity ${idx + 1}] (${e.type || 'Unknown'}) ${e.name}: ${clipText(e.summary || '', 400)}`
      ).join('\n');

      const factLines = entityFacts.slice(0, 6).map((f: any, idx: number) =>
          `- [Entity Fact ${idx + 1}] (${f.type || 'Unknown'}) ${f.name}: ${clipText(f.fact, 400)}`
      ).join('\n');

    const contextBlock = `MASTER MEMORY:\n${masterRelevant || masterFallback ? clipText(masterContent || '(none)', 500) : '(not relevant)'}\n\nRELEVANT MEMORIES:\n${memoryLines || '(none)'}\n\nRELEVANT MESSAGES:\n${messageLines || '(none)'}\n\nRELEVANT SUMMARIES:\n${summaryLines || '(none)'}\n\nRELEVANT ENTITIES:\n${entityLines || '(none)'}\n\nRELEVANT ENTITY FACTS:\n${factLines || '(none)'}`;

    // Build conversation history block if provided
    let historyBlock = '';
    if (conversationHistory && conversationHistory.length > 0) {
        const historyLines = conversationHistory.map(msg => 
            `${msg.role === 'user' ? 'User' : 'Assistant'}: ${clipText(msg.content, 500)}`
        ).join('\n\n');
        historyBlock = `\nCONVERSATION HISTORY:\n${historyLines}\n`;
    }

    const prompt = getPrompt('ragChat', {
        HISTORY_BLOCK: historyBlock,
        QUERY: query,
        CONTEXT_BLOCK: contextBlock,
    });

      try {
          const { llm } = await import('./llm');
          const answer = await llm.chat(prompt, [], 300000, 2048, 'Memory Chat');
          return {
              answer,
              context: {
                  masterMemory: masterRelevant || masterFallback ? masterContent : null,
                  memories,
                  messages,
                  summaries,
                  entities,
                  entityFacts
              }
          };
      } catch (e) {
          console.error('[RAG] LLM chat failed:', e);
          return {
              answer: 'Sorry, I could not generate a response right now.',
              context: {
                  masterMemory: masterRelevant || masterFallback ? masterContent : null,
                  memories,
                  messages,
                  summaries,
                  entities,
                  entityFacts
              }
          };
      }
  });

  ipcMain.handle('db:get-chat-sessions', (_, appName?: string) => {
      return getChatSessions(appName);
  });
  ipcMain.handle('db:get-memories-for-session', (_, sessionId: string) => {
      // Need to export this from database.ts first or import it
      return getMemoriesForSession(sessionId);
  });
  ipcMain.handle('db:get-entities', (_, appName?: string) => {
      return getEntities(appName);
  });
  ipcMain.handle('db:get-entity-details', (_, entityId: number, appName?: string) => {
      return getEntityDetails(entityId, appName);
  });
  ipcMain.handle('db:get-entities-for-session', (_, sessionId: string) => {
      return getEntitiesForSession(sessionId);
  });
  ipcMain.handle('db:get-memory-records-for-session', (_, sessionId: string) => {
      return getMemoryRecordsForSession(sessionId);
  });

  ipcMain.handle('db:get-entity-graph', (_, appName?: string, focusEntityId?: number, edgeLimit: number = 200) => {
      return getEntityGraph(appName, focusEntityId, edgeLimit);
  });

  ipcMain.handle('db:rebuild-entity-graph', () => {
      rebuildEntityEdgesForAllSessions();
      return true;
  });
  ipcMain.handle('db:delete-session', async (_, sessionId: string) => {
      const db = getDB();
      // Delete from new tables (messages will cascade due to foreign key)
      db.prepare('DELETE FROM conversations WHERE id = ?').run(sessionId);
      // Also delete from legacy tables for cleanup
      db.prepare('DELETE FROM memories WHERE session_id = ?').run(sessionId);
      db.prepare('DELETE FROM chat_summaries WHERE session_id = ?').run(sessionId);
      console.log(`Deleted session: ${sessionId}`);
      
      // Regenerate master memory after deletion
      await regenerateMasterMemory();
      
      return true;
  });

  ipcMain.handle('llm:summarize-session', async (_, sessionId: string) => {
      return await summarizeSession(sessionId);
  });

  ipcMain.handle('db:get-master-memory', () => {
      return getMasterMemory();
  });

  ipcMain.handle('db:delete-entity', (_, entityId: number) => {
      const result = deleteEntity(entityId);
      console.log(`[IPC] Deleted entity ${entityId}: ${result}`);
      return result;
  });

  ipcMain.handle('db:delete-memory', (_, memoryId: number) => {
      const result = deleteMemory(memoryId);
      console.log(`[IPC] Deleted memory ${memoryId}: ${result}`);
      return result;
  });

  ipcMain.handle('db:regenerate-master-memory', async () => {
      return await regenerateMasterMemory();
  });

  // User Profile handlers
  ipcMain.handle('db:get-user-profile', () => {
      return getUserProfile();
  });

  ipcMain.handle('db:save-user-profile', (_, profile: UserProfile) => {
      saveUserProfile(profile);
      console.log('[IPC] User profile saved:', profile);
      return true;
  });

  // Permission handlers
  ipcMain.handle('permissions:get-status', () => {
      return getPermissionStatus();
  });

  ipcMain.handle('permissions:request-accessibility', () => {
      return requestAccessibilityPermission();
  });

  ipcMain.handle('permissions:open-accessibility-settings', () => {
      openAccessibilitySettings();
      return true;
  });

  ipcMain.handle('permissions:open-screen-recording-settings', () => {
      openScreenRecordingSettings();
      return true;
  });

  ipcMain.handle('permissions:request-screen-recording', async () => {
      return await requestScreenRecordingPermission();
  });

  // === RAG CONVERSATION HANDLERS ===
  
  ipcMain.handle('rag:create-conversation', (_, id: string, title?: string) => {
      return createRagConversation(id, title);
  });

  ipcMain.handle('rag:get-conversations', () => {
      return getRagConversations();
  });

  ipcMain.handle('rag:get-conversation', (_, id: string) => {
      return getRagConversation(id);
  });

  ipcMain.handle('rag:get-messages', (_, conversationId: string) => {
      return getRagMessages(conversationId);
  });

  ipcMain.handle('rag:add-message', (_, conversationId: string, role: 'user' | 'assistant', content: string, context?: any) => {
      return addRagMessage(conversationId, role, content, context);
  });

  ipcMain.handle('rag:update-conversation-title', (_, id: string, title: string) => {
      updateRagConversationTitle(id, title);
      return true;
  });

  ipcMain.handle('rag:delete-conversation', (_, id: string) => {
      return deleteRagConversation(id);
  });

  // === SETTINGS HANDLERS ===
  
  ipcMain.handle('settings:get', () => {
      return getSettings();
  });

  ipcMain.handle('settings:save', (_, key: string, value: any) => {
      saveSetting(key, value);
      console.log(`[IPC] Setting saved: ${key} =`, value);
      return true;
  });

  // === PROMPT HANDLERS ===

  ipcMain.handle('prompts:get-all', () => {
      const defs = getAllPromptDefs();
      return defs.map(def => ({
          ...def,
          currentTemplate: getPromptTemplate(def.key) !== def.defaultTemplate ? getPromptTemplate(def.key) : null,
      }));
  });

  ipcMain.handle('prompts:save', (_, key: string, value: string) => {
      saveSetting(`prompt:${key}`, value);
      console.log(`[IPC] Prompt saved: ${key}`);
      return true;
  });

  ipcMain.handle('prompts:reset', (_, key: string) => {
      resetPrompt(key);
      console.log(`[IPC] Prompt reset: ${key}`);
      return true;
  });

  // === REPROCESS ALL SESSIONS ===

  ipcMain.handle('db:reprocess-all-sessions', async (_, clean: boolean = false) => {
      const db = getDB();
      const sessions = db.prepare('SELECT id FROM conversations').all() as { id: string }[];
      let processed = 0;

      if (clean) {
          // Clean reprocess: delete all old data and rebuild from scratch
          console.log('[IPC] Clean reprocess: clearing all entities, facts, edges, and memories...');

          // Drop FTS AFTER DELETE triggers first — if the FTS index is out of sync
          // with the source tables, the delete triggers will error and silently
          // prevent rows from being deleted. We recreate them after.
          db.exec('DROP TRIGGER IF EXISTS memories_ad');
          db.exec('DROP TRIGGER IF EXISTS entities_ad');
          db.exec('DROP TRIGGER IF EXISTS entity_facts_ad');

          // Delete only strictness-dependent data (children first)
          // Conversations, messages, chat_summaries, and master_memory are NOT touched
          db.prepare('DELETE FROM entity_edges').run();
          db.prepare('DELETE FROM entity_facts').run();
          db.prepare('DELETE FROM entity_sessions').run();
          db.prepare('DELETE FROM entities').run();
          db.prepare('DELETE FROM memories').run();

          // Recreate the delete triggers
          db.exec(`CREATE TRIGGER IF NOT EXISTS memories_ad AFTER DELETE ON memories BEGIN
              INSERT INTO memory_fts(memory_fts, rowid, content) VALUES('delete', old.id, old.content);
          END;`);
          db.exec(`CREATE TRIGGER IF NOT EXISTS entities_ad AFTER DELETE ON entities BEGIN
              INSERT INTO entity_fts(entity_fts, rowid, name, summary, type) VALUES('delete', old.id, old.name, old.summary, old.type);
          END;`);
          db.exec(`CREATE TRIGGER IF NOT EXISTS entity_facts_ad AFTER DELETE ON entity_facts BEGIN
              INSERT INTO entity_fact_fts(entity_fact_fts, rowid, fact, entity_id) VALUES('delete', old.id, old.fact, old.entity_id);
          END;`);

          // Rebuild FTS indexes so they reflect the now-empty source tables
          try {
              db.exec("INSERT INTO memory_fts(memory_fts) VALUES('rebuild')");
              db.exec("INSERT INTO entity_fts(entity_fts) VALUES('rebuild')");
              db.exec("INSERT INTO entity_fact_fts(entity_fact_fts) VALUES('rebuild')");
          } catch (e) {
              console.error('[IPC] FTS rebuild during clean reprocess failed (non-fatal):', e);
          }

          const deletedCounts = {
              entities: (db.prepare('SELECT COUNT(*) as c FROM entities').get() as { c: number }).c,
              memories: (db.prepare('SELECT COUNT(*) as c FROM memories').get() as { c: number }).c,
              facts: (db.prepare('SELECT COUNT(*) as c FROM entity_facts').get() as { c: number }).c,
          };
          console.log('[IPC] Post-delete counts (should all be 0):', deletedCounts);

          // Notify frontend to refresh immediately after clearing
          BrowserWindow.getAllWindows().forEach(win => {
              win.webContents.send('reprocess:progress', { phase: 'cleared', processed: 0, total: sessions.length });
          });

          for (const session of sessions) {
              try {
                  // Re-evaluate memories for each message in the session
                  const msgs = db.prepare('SELECT id, role, content FROM messages WHERE conversation_id = ? ORDER BY created_at ASC').all(session.id) as { id: number; role: string; content: string }[];
                  const conv = db.prepare('SELECT app_name FROM conversations WHERE id = ?').get(session.id) as { app_name: string } | undefined;
                  const appName = conv?.app_name || 'Unknown';

                  for (const msg of msgs) {
                      await evaluateAndStoreMemoryForMessage({
                          sessionId: session.id,
                          appName,
                          role: msg.role,
                          content: msg.content,
                          messageId: msg.id
                      });
                  }

                  // Re-extract entities from the newly created memories
                  await extractEntitiesForSession(session.id);
                  processed++;

                  // Send progress updates
                  BrowserWindow.getAllWindows().forEach(win => {
                      win.webContents.send('reprocess:progress', { phase: 'processing', processed, total: sessions.length });
                  });
              } catch (e) {
                  console.error(`[IPC] Failed to reprocess session ${session.id}:`, e);
              }
          }

          // Rebuild entity edges from mentions across all entities
          rebuildEntityEdgesForAllSessions();
      } else {
          // Additive reprocess: keep existing data, just re-run entity extraction on top
          console.log('[IPC] Additive reprocess: re-extracting entities with current settings (keeping existing data)...');
          for (const session of sessions) {
              try {
                  await extractEntitiesForSession(session.id);
                  processed++;
              } catch (e) {
                  console.error(`[IPC] Failed to reprocess session ${session.id}:`, e);
              }
          }
      }

      console.log(`[IPC] Reprocessed ${processed} sessions (clean=${clean}) with current strictness settings`);
      return { processed, total: sessions.length };
  });

  // === MODEL DOWNLOAD HANDLERS ===
  
  ipcMain.handle('model:check-status', async () => {
      const { llm } = await import('./llm');
      return {
          downloaded: llm.modelsExist(),
          modelsDir: llm.getModelsDir()
      };
  });

  ipcMain.handle('model:download', async () => {
      const { llm } = await import('./llm');
      const modelsDir = llm.getModelsDir();
      const fs = await import('fs');
      const path = await import('path');
      const https = await import('https');
      
      // Ensure models directory exists
      if (!fs.existsSync(modelsDir)) {
          fs.mkdirSync(modelsDir, { recursive: true });
      }

      const models = [
          {
              name: 'Qwen3-VL-4B-Instruct-Q4_K_M.gguf',
              url: 'https://huggingface.co/bartowski/Qwen_Qwen3-VL-4B-Instruct-GGUF/resolve/main/Qwen_Qwen3-VL-4B-Instruct-Q4_K_M.gguf'
          },
          {
              name: 'mmproj-Qwen3VL-4B-Instruct-F16.gguf',
              url: 'https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-GGUF/resolve/main/mmproj-Qwen3VL-4B-Instruct-F16.gguf'
          }
      ];

      const downloadFile = (url: string, destPath: string, modelName: string): Promise<void> => {
          return new Promise((resolve, reject) => {
              const file = fs.createWriteStream(destPath);
              
              const request = (redirectUrl: string) => {
                  https.get(redirectUrl, (response) => {
                      if (response.statusCode && response.statusCode >= 300 && response.statusCode < 400 && response.headers.location) {
                          request(response.headers.location);
                          return;
                      }
                      
                      if (response.statusCode !== 200) {
                          fs.unlink(destPath, () => {});
                          reject(new Error(`HTTP ${response.statusCode}`));
                          return;
                      }
                      
                      const totalSize = parseInt(response.headers['content-length'] || '0', 10);
                      let downloaded = 0;
                      
                      response.pipe(file);
                      
                      response.on('data', (chunk: Buffer) => {
                          downloaded += chunk.length;
                          const percent = totalSize ? Math.round((downloaded / totalSize) * 100) : 0;
                          
                          // Send progress to renderer
                          BrowserWindow.getAllWindows().forEach(win => {
                              win.webContents.send('model:download-progress', {
                                  modelName,
                                  percent,
                                  downloadedMB: (downloaded / 1024 / 1024).toFixed(1),
                                  totalMB: totalSize ? (totalSize / 1024 / 1024).toFixed(1) : '?'
                              });
                          });
                      });
                      
                      file.on('finish', () => {
                          file.close();
                          resolve();
                      });
                  }).on('error', (err) => {
                      fs.unlink(destPath, () => {});
                      reject(err);
                  });
              };
              
              request(url);
          });
      };

      try {
          for (const model of models) {
              const destPath = path.join(modelsDir, model.name);
              
              if (fs.existsSync(destPath)) {
                  console.log(`[Model] ${model.name} already exists, skipping`);
                  continue;
              }
              
              console.log(`[Model] Downloading ${model.name}...`);
              await downloadFile(model.url, destPath, model.name);
              console.log(`[Model] ${model.name} downloaded`);
          }
          
          return { success: true };
      } catch (err: any) {
          console.error('[Model] Download failed:', err);
          return { success: false, error: err.message };
      }
  });
}

